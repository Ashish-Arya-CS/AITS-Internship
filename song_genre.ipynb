{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/music-genre-classification/dataset.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"datatrain = pd.read_csv(\"/kaggle/input/music-genre-classification/dataset.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datatrain.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"          filename  chroma_stft      rmse  spectral_centroid  \\\n0  blues.00000.wav     0.349943  0.130225        1784.420446   \n1  blues.00001.wav     0.340983  0.095918        1529.835316   \n2  blues.00002.wav     0.363603  0.175573        1552.481958   \n3  blues.00003.wav     0.404779  0.141191        1070.119953   \n4  blues.00004.wav     0.308590  0.091563        1835.494603   \n\n   spectral_bandwidth      rolloff  zero_crossing_rate       mfcc1  \\\n0         2002.650192  3806.485316            0.083066 -113.596748   \n1         2038.617579  3548.820207            0.056044 -207.556793   \n2         1747.165985  3040.514948            0.076301  -90.754387   \n3         1596.333948  2185.028454            0.033309 -199.431152   \n4         1748.362448  3580.945013            0.101500 -160.266037   \n\n        mfcc2      mfcc3  ...    mfcc12    mfcc13    mfcc14    mfcc15  \\\n0  121.557297 -19.158825  ...  8.810669 -3.667368  5.751691 -5.162763   \n1  124.006721   8.930560  ...  5.376803 -2.239120  4.216963 -6.012273   \n2  140.459900 -29.109968  ...  5.789265 -8.905224 -1.083720 -9.218359   \n3  150.099213   5.647593  ...  6.087677 -2.476421 -1.073890 -2.874778   \n4  126.198807 -35.605450  ... -2.806384 -6.934123 -7.558618 -9.173553   \n\n     mfcc16    mfcc17    mfcc18    mfcc19     mfcc20  label  \n0  0.750948 -1.691938 -0.409953 -2.300209   1.219929  blues  \n1  0.936110 -0.716537  0.293876 -0.287431   0.531573  blues  \n2  2.455806 -7.726901 -1.815723 -3.433434  -2.226821  blues  \n3  0.780977 -3.316932  0.637982 -0.619690  -3.408233  blues  \n4 -4.512165 -5.453538 -0.924161 -4.409333 -11.703781  blues  \n\n[5 rows x 28 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>chroma_stft</th>\n      <th>rmse</th>\n      <th>spectral_centroid</th>\n      <th>spectral_bandwidth</th>\n      <th>rolloff</th>\n      <th>zero_crossing_rate</th>\n      <th>mfcc1</th>\n      <th>mfcc2</th>\n      <th>mfcc3</th>\n      <th>...</th>\n      <th>mfcc12</th>\n      <th>mfcc13</th>\n      <th>mfcc14</th>\n      <th>mfcc15</th>\n      <th>mfcc16</th>\n      <th>mfcc17</th>\n      <th>mfcc18</th>\n      <th>mfcc19</th>\n      <th>mfcc20</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>blues.00000.wav</td>\n      <td>0.349943</td>\n      <td>0.130225</td>\n      <td>1784.420446</td>\n      <td>2002.650192</td>\n      <td>3806.485316</td>\n      <td>0.083066</td>\n      <td>-113.596748</td>\n      <td>121.557297</td>\n      <td>-19.158825</td>\n      <td>...</td>\n      <td>8.810669</td>\n      <td>-3.667368</td>\n      <td>5.751691</td>\n      <td>-5.162763</td>\n      <td>0.750948</td>\n      <td>-1.691938</td>\n      <td>-0.409953</td>\n      <td>-2.300209</td>\n      <td>1.219929</td>\n      <td>blues</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>blues.00001.wav</td>\n      <td>0.340983</td>\n      <td>0.095918</td>\n      <td>1529.835316</td>\n      <td>2038.617579</td>\n      <td>3548.820207</td>\n      <td>0.056044</td>\n      <td>-207.556793</td>\n      <td>124.006721</td>\n      <td>8.930560</td>\n      <td>...</td>\n      <td>5.376803</td>\n      <td>-2.239120</td>\n      <td>4.216963</td>\n      <td>-6.012273</td>\n      <td>0.936110</td>\n      <td>-0.716537</td>\n      <td>0.293876</td>\n      <td>-0.287431</td>\n      <td>0.531573</td>\n      <td>blues</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>blues.00002.wav</td>\n      <td>0.363603</td>\n      <td>0.175573</td>\n      <td>1552.481958</td>\n      <td>1747.165985</td>\n      <td>3040.514948</td>\n      <td>0.076301</td>\n      <td>-90.754387</td>\n      <td>140.459900</td>\n      <td>-29.109968</td>\n      <td>...</td>\n      <td>5.789265</td>\n      <td>-8.905224</td>\n      <td>-1.083720</td>\n      <td>-9.218359</td>\n      <td>2.455806</td>\n      <td>-7.726901</td>\n      <td>-1.815723</td>\n      <td>-3.433434</td>\n      <td>-2.226821</td>\n      <td>blues</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>blues.00003.wav</td>\n      <td>0.404779</td>\n      <td>0.141191</td>\n      <td>1070.119953</td>\n      <td>1596.333948</td>\n      <td>2185.028454</td>\n      <td>0.033309</td>\n      <td>-199.431152</td>\n      <td>150.099213</td>\n      <td>5.647593</td>\n      <td>...</td>\n      <td>6.087677</td>\n      <td>-2.476421</td>\n      <td>-1.073890</td>\n      <td>-2.874778</td>\n      <td>0.780977</td>\n      <td>-3.316932</td>\n      <td>0.637982</td>\n      <td>-0.619690</td>\n      <td>-3.408233</td>\n      <td>blues</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>blues.00004.wav</td>\n      <td>0.308590</td>\n      <td>0.091563</td>\n      <td>1835.494603</td>\n      <td>1748.362448</td>\n      <td>3580.945013</td>\n      <td>0.101500</td>\n      <td>-160.266037</td>\n      <td>126.198807</td>\n      <td>-35.605450</td>\n      <td>...</td>\n      <td>-2.806384</td>\n      <td>-6.934123</td>\n      <td>-7.558618</td>\n      <td>-9.173553</td>\n      <td>-4.512165</td>\n      <td>-5.453538</td>\n      <td>-0.924161</td>\n      <td>-4.409333</td>\n      <td>-11.703781</td>\n      <td>blues</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"datatrain = datatrain.set_index(\"filename\")","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datatrain.describe()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"       chroma_stft         rmse  spectral_centroid  spectral_bandwidth  \\\ncount  1000.000000  1000.000000        1000.000000         1000.000000   \nmean      0.378669     0.130929        2201.834226         2242.559613   \nstd       0.081706     0.065685         715.961347          526.337663   \nmin       0.171782     0.005276         569.930721          897.994319   \n25%       0.319641     0.086625        1627.793931         1907.136505   \n50%       0.383075     0.122448        2209.468780         2221.408983   \n75%       0.435974     0.175793        2691.969702         2578.474352   \nmax       0.663573     0.398012        4434.439444         3509.578677   \n\n           rolloff  zero_crossing_rate        mfcc1        mfcc2        mfcc3  \\\ncount  1000.000000         1000.000000  1000.000000  1000.000000  1000.000000   \nmean   4571.702159            0.103637  -144.479170    99.552199    -8.921949   \nstd    1574.770035            0.041834   100.235659    31.331904    21.695015   \nmin     749.062137            0.021701  -552.064026    -1.527147   -89.901138   \n25%    3380.956639            0.070281  -200.695129    76.811485   -24.223789   \n50%    4658.671830            0.099539  -120.206070    98.452553   -10.716073   \n75%    5534.197785            0.132007   -73.895018   119.893629     5.505793   \nmax    8676.405868            0.274829    42.034588   193.096512    56.666088   \n\n             mfcc4  ...       mfcc11       mfcc12       mfcc13       mfcc14  \\\ncount  1000.000000  ...  1000.000000  1000.000000  1000.000000  1000.000000   \nmean     36.293061  ...    -6.021121     4.471604    -4.797232     1.781547   \nstd      16.666986  ...     6.819018     6.717312     6.170919     5.009489   \nmin     -18.768461  ...   -28.052265   -15.805225   -27.542309   -12.598773   \n25%      24.107393  ...   -10.966826    -0.551576    -9.363372    -1.640080   \n50%      36.957073  ...    -5.920161     3.891842    -4.199810     1.879423   \n75%      48.212825  ...    -1.004241     9.706133    -0.161017     5.155263   \nmax      80.691277  ...    17.421038    23.037573    13.054334    18.161661   \n\n            mfcc15       mfcc16       mfcc17       mfcc18       mfcc19  \\\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \nmean     -3.870276     1.147988    -3.967431     0.507303    -2.328779   \nstd       4.874423     4.579110     4.550650     3.869088     3.755574   \nmin     -17.545473   -15.693589   -17.227766   -11.975698   -18.504187   \n25%      -7.164838    -1.857098    -7.194296    -2.003978    -4.670281   \n50%      -3.614473     1.211945    -4.059109     0.669789    -2.391261   \n75%      -0.323536     4.350694    -0.842968     3.112519     0.149070   \nmax      12.357588    13.468802    11.489994    15.379257    14.686911   \n\n            mfcc20  \ncount  1000.000000  \nmean     -1.094875  \nstd       3.837561  \nmin     -19.935202  \n25%      -3.367999  \n50%      -1.155198  \n75%       1.303739  \nmax      15.368967  \n\n[8 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chroma_stft</th>\n      <th>rmse</th>\n      <th>spectral_centroid</th>\n      <th>spectral_bandwidth</th>\n      <th>rolloff</th>\n      <th>zero_crossing_rate</th>\n      <th>mfcc1</th>\n      <th>mfcc2</th>\n      <th>mfcc3</th>\n      <th>mfcc4</th>\n      <th>...</th>\n      <th>mfcc11</th>\n      <th>mfcc12</th>\n      <th>mfcc13</th>\n      <th>mfcc14</th>\n      <th>mfcc15</th>\n      <th>mfcc16</th>\n      <th>mfcc17</th>\n      <th>mfcc18</th>\n      <th>mfcc19</th>\n      <th>mfcc20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>...</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.378669</td>\n      <td>0.130929</td>\n      <td>2201.834226</td>\n      <td>2242.559613</td>\n      <td>4571.702159</td>\n      <td>0.103637</td>\n      <td>-144.479170</td>\n      <td>99.552199</td>\n      <td>-8.921949</td>\n      <td>36.293061</td>\n      <td>...</td>\n      <td>-6.021121</td>\n      <td>4.471604</td>\n      <td>-4.797232</td>\n      <td>1.781547</td>\n      <td>-3.870276</td>\n      <td>1.147988</td>\n      <td>-3.967431</td>\n      <td>0.507303</td>\n      <td>-2.328779</td>\n      <td>-1.094875</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.081706</td>\n      <td>0.065685</td>\n      <td>715.961347</td>\n      <td>526.337663</td>\n      <td>1574.770035</td>\n      <td>0.041834</td>\n      <td>100.235659</td>\n      <td>31.331904</td>\n      <td>21.695015</td>\n      <td>16.666986</td>\n      <td>...</td>\n      <td>6.819018</td>\n      <td>6.717312</td>\n      <td>6.170919</td>\n      <td>5.009489</td>\n      <td>4.874423</td>\n      <td>4.579110</td>\n      <td>4.550650</td>\n      <td>3.869088</td>\n      <td>3.755574</td>\n      <td>3.837561</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.171782</td>\n      <td>0.005276</td>\n      <td>569.930721</td>\n      <td>897.994319</td>\n      <td>749.062137</td>\n      <td>0.021701</td>\n      <td>-552.064026</td>\n      <td>-1.527147</td>\n      <td>-89.901138</td>\n      <td>-18.768461</td>\n      <td>...</td>\n      <td>-28.052265</td>\n      <td>-15.805225</td>\n      <td>-27.542309</td>\n      <td>-12.598773</td>\n      <td>-17.545473</td>\n      <td>-15.693589</td>\n      <td>-17.227766</td>\n      <td>-11.975698</td>\n      <td>-18.504187</td>\n      <td>-19.935202</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.319641</td>\n      <td>0.086625</td>\n      <td>1627.793931</td>\n      <td>1907.136505</td>\n      <td>3380.956639</td>\n      <td>0.070281</td>\n      <td>-200.695129</td>\n      <td>76.811485</td>\n      <td>-24.223789</td>\n      <td>24.107393</td>\n      <td>...</td>\n      <td>-10.966826</td>\n      <td>-0.551576</td>\n      <td>-9.363372</td>\n      <td>-1.640080</td>\n      <td>-7.164838</td>\n      <td>-1.857098</td>\n      <td>-7.194296</td>\n      <td>-2.003978</td>\n      <td>-4.670281</td>\n      <td>-3.367999</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.383075</td>\n      <td>0.122448</td>\n      <td>2209.468780</td>\n      <td>2221.408983</td>\n      <td>4658.671830</td>\n      <td>0.099539</td>\n      <td>-120.206070</td>\n      <td>98.452553</td>\n      <td>-10.716073</td>\n      <td>36.957073</td>\n      <td>...</td>\n      <td>-5.920161</td>\n      <td>3.891842</td>\n      <td>-4.199810</td>\n      <td>1.879423</td>\n      <td>-3.614473</td>\n      <td>1.211945</td>\n      <td>-4.059109</td>\n      <td>0.669789</td>\n      <td>-2.391261</td>\n      <td>-1.155198</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.435974</td>\n      <td>0.175793</td>\n      <td>2691.969702</td>\n      <td>2578.474352</td>\n      <td>5534.197785</td>\n      <td>0.132007</td>\n      <td>-73.895018</td>\n      <td>119.893629</td>\n      <td>5.505793</td>\n      <td>48.212825</td>\n      <td>...</td>\n      <td>-1.004241</td>\n      <td>9.706133</td>\n      <td>-0.161017</td>\n      <td>5.155263</td>\n      <td>-0.323536</td>\n      <td>4.350694</td>\n      <td>-0.842968</td>\n      <td>3.112519</td>\n      <td>0.149070</td>\n      <td>1.303739</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.663573</td>\n      <td>0.398012</td>\n      <td>4434.439444</td>\n      <td>3509.578677</td>\n      <td>8676.405868</td>\n      <td>0.274829</td>\n      <td>42.034588</td>\n      <td>193.096512</td>\n      <td>56.666088</td>\n      <td>80.691277</td>\n      <td>...</td>\n      <td>17.421038</td>\n      <td>23.037573</td>\n      <td>13.054334</td>\n      <td>18.161661</td>\n      <td>12.357588</td>\n      <td>13.468802</td>\n      <td>11.489994</td>\n      <td>15.379257</td>\n      <td>14.686911</td>\n      <td>15.368967</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 26 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"datatrain.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"                 chroma_stft      rmse  spectral_centroid  spectral_bandwidth  \\\nfilename                                                                        \nblues.00000.wav     0.349943  0.130225        1784.420446         2002.650192   \nblues.00001.wav     0.340983  0.095918        1529.835316         2038.617579   \nblues.00002.wav     0.363603  0.175573        1552.481958         1747.165985   \nblues.00003.wav     0.404779  0.141191        1070.119953         1596.333948   \nblues.00004.wav     0.308590  0.091563        1835.494603         1748.362448   \n\n                     rolloff  zero_crossing_rate       mfcc1       mfcc2  \\\nfilename                                                                   \nblues.00000.wav  3806.485316            0.083066 -113.596748  121.557297   \nblues.00001.wav  3548.820207            0.056044 -207.556793  124.006721   \nblues.00002.wav  3040.514948            0.076301  -90.754387  140.459900   \nblues.00003.wav  2185.028454            0.033309 -199.431152  150.099213   \nblues.00004.wav  3580.945013            0.101500 -160.266037  126.198807   \n\n                     mfcc3      mfcc4  ...    mfcc12    mfcc13    mfcc14  \\\nfilename                               ...                                 \nblues.00000.wav -19.158825  42.351028  ...  8.810669 -3.667368  5.751691   \nblues.00001.wav   8.930560  35.874683  ...  5.376803 -2.239120  4.216963   \nblues.00002.wav -29.109968  31.689013  ...  5.789265 -8.905224 -1.083720   \nblues.00003.wav   5.647593  26.871927  ...  6.087677 -2.476421 -1.073890   \nblues.00004.wav -35.605450  22.153301  ... -2.806384 -6.934123 -7.558618   \n\n                   mfcc15    mfcc16    mfcc17    mfcc18    mfcc19     mfcc20  \\\nfilename                                                                       \nblues.00000.wav -5.162763  0.750948 -1.691938 -0.409953 -2.300209   1.219929   \nblues.00001.wav -6.012273  0.936110 -0.716537  0.293876 -0.287431   0.531573   \nblues.00002.wav -9.218359  2.455806 -7.726901 -1.815723 -3.433434  -2.226821   \nblues.00003.wav -2.874778  0.780977 -3.316932  0.637982 -0.619690  -3.408233   \nblues.00004.wav -9.173553 -4.512165 -5.453538 -0.924161 -4.409333 -11.703781   \n\n                 label  \nfilename                \nblues.00000.wav  blues  \nblues.00001.wav  blues  \nblues.00002.wav  blues  \nblues.00003.wav  blues  \nblues.00004.wav  blues  \n\n[5 rows x 27 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chroma_stft</th>\n      <th>rmse</th>\n      <th>spectral_centroid</th>\n      <th>spectral_bandwidth</th>\n      <th>rolloff</th>\n      <th>zero_crossing_rate</th>\n      <th>mfcc1</th>\n      <th>mfcc2</th>\n      <th>mfcc3</th>\n      <th>mfcc4</th>\n      <th>...</th>\n      <th>mfcc12</th>\n      <th>mfcc13</th>\n      <th>mfcc14</th>\n      <th>mfcc15</th>\n      <th>mfcc16</th>\n      <th>mfcc17</th>\n      <th>mfcc18</th>\n      <th>mfcc19</th>\n      <th>mfcc20</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>filename</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>blues.00000.wav</th>\n      <td>0.349943</td>\n      <td>0.130225</td>\n      <td>1784.420446</td>\n      <td>2002.650192</td>\n      <td>3806.485316</td>\n      <td>0.083066</td>\n      <td>-113.596748</td>\n      <td>121.557297</td>\n      <td>-19.158825</td>\n      <td>42.351028</td>\n      <td>...</td>\n      <td>8.810669</td>\n      <td>-3.667368</td>\n      <td>5.751691</td>\n      <td>-5.162763</td>\n      <td>0.750948</td>\n      <td>-1.691938</td>\n      <td>-0.409953</td>\n      <td>-2.300209</td>\n      <td>1.219929</td>\n      <td>blues</td>\n    </tr>\n    <tr>\n      <th>blues.00001.wav</th>\n      <td>0.340983</td>\n      <td>0.095918</td>\n      <td>1529.835316</td>\n      <td>2038.617579</td>\n      <td>3548.820207</td>\n      <td>0.056044</td>\n      <td>-207.556793</td>\n      <td>124.006721</td>\n      <td>8.930560</td>\n      <td>35.874683</td>\n      <td>...</td>\n      <td>5.376803</td>\n      <td>-2.239120</td>\n      <td>4.216963</td>\n      <td>-6.012273</td>\n      <td>0.936110</td>\n      <td>-0.716537</td>\n      <td>0.293876</td>\n      <td>-0.287431</td>\n      <td>0.531573</td>\n      <td>blues</td>\n    </tr>\n    <tr>\n      <th>blues.00002.wav</th>\n      <td>0.363603</td>\n      <td>0.175573</td>\n      <td>1552.481958</td>\n      <td>1747.165985</td>\n      <td>3040.514948</td>\n      <td>0.076301</td>\n      <td>-90.754387</td>\n      <td>140.459900</td>\n      <td>-29.109968</td>\n      <td>31.689013</td>\n      <td>...</td>\n      <td>5.789265</td>\n      <td>-8.905224</td>\n      <td>-1.083720</td>\n      <td>-9.218359</td>\n      <td>2.455806</td>\n      <td>-7.726901</td>\n      <td>-1.815723</td>\n      <td>-3.433434</td>\n      <td>-2.226821</td>\n      <td>blues</td>\n    </tr>\n    <tr>\n      <th>blues.00003.wav</th>\n      <td>0.404779</td>\n      <td>0.141191</td>\n      <td>1070.119953</td>\n      <td>1596.333948</td>\n      <td>2185.028454</td>\n      <td>0.033309</td>\n      <td>-199.431152</td>\n      <td>150.099213</td>\n      <td>5.647593</td>\n      <td>26.871927</td>\n      <td>...</td>\n      <td>6.087677</td>\n      <td>-2.476421</td>\n      <td>-1.073890</td>\n      <td>-2.874778</td>\n      <td>0.780977</td>\n      <td>-3.316932</td>\n      <td>0.637982</td>\n      <td>-0.619690</td>\n      <td>-3.408233</td>\n      <td>blues</td>\n    </tr>\n    <tr>\n      <th>blues.00004.wav</th>\n      <td>0.308590</td>\n      <td>0.091563</td>\n      <td>1835.494603</td>\n      <td>1748.362448</td>\n      <td>3580.945013</td>\n      <td>0.101500</td>\n      <td>-160.266037</td>\n      <td>126.198807</td>\n      <td>-35.605450</td>\n      <td>22.153301</td>\n      <td>...</td>\n      <td>-2.806384</td>\n      <td>-6.934123</td>\n      <td>-7.558618</td>\n      <td>-9.173553</td>\n      <td>-4.512165</td>\n      <td>-5.453538</td>\n      <td>-0.924161</td>\n      <td>-4.409333</td>\n      <td>-11.703781</td>\n      <td>blues</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 27 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n# creating instance of labelencoder\n#labelencoder = LabelEncoder()\n# Assigning numerical values and storing in another column\n#datatrain['label'] = labelencoder.fit_transform(datatrain[\"label\"])\n#datatrain","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = datatrain.iloc[: ,:-1].values\nY = datatrain['label'].values","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As this is a multiclass classification problem onehotencoding our Y.\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"((750, 26), (750, 10), (250, 26), (250, 10))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n# scaling our data with sklearn's Standard scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"((750, 26), (750, 10), (250, 26), (250, 10))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making our data compatible to model.\nx_train = np.expand_dims(x_train, axis=2)\nx_test = np.expand_dims(x_test, axis=2)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"((750, 26, 1), (750, 10), (250, 26, 1), (250, 10))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization ,Activation\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[0].shape","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"(26, 1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(512, activation = 'relu', input_shape = x_train[0].shape))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(64, activation='relu')) \nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(10, activation='softmax')) \nmodel.summary()","execution_count":54,"outputs":[{"output_type":"stream","text":"Model: \"sequential_14\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_43 (Dense)             (None, 512)               13824     \n_________________________________________________________________\ndropout_36 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_44 (Dense)             (None, 256)               131328    \n_________________________________________________________________\ndropout_37 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_45 (Dense)             (None, 64)                16448     \n_________________________________________________________________\ndropout_38 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_46 (Dense)             (None, 10)                650       \n=================================================================\nTotal params: 162,250\nTrainable params: 162,250\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer= opt ,loss='categorical_crossentropy',metrics=['acc'])","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building the model:\nmodel = Sequential()\nmodel.add(Conv1D(64, 8, padding='same',activation = 'relu',input_shape=(x_train.shape[1],1)))  \nmodel.add(Conv1D(64, 8, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(MaxPooling1D(pool_size=(3)))\nmodel.add(Conv1D(32, 8, padding='same', activation='relu'))\nmodel.add(Conv1D(32, 8, padding='same', activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Conv1D(16, 8, padding='same', activation='relu'))\nmodel.add(Conv1D(16, 8, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(MaxPooling1D(pool_size=(3)))\nmodel.add(Conv1D(8, 8, padding='same', activation='relu'))\nmodel.add(Conv1D(8, 8, padding='same', activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='softmax')) \nopt = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer= opt ,loss='categorical_crossentropy',metrics=['acc'])\n\nmodel.summary()\n","execution_count":19,"outputs":[{"output_type":"stream","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_22 (Conv1D)           (None, 26, 64)            576       \n_________________________________________________________________\nconv1d_23 (Conv1D)           (None, 26, 64)            32832     \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 26, 64)            256       \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 26, 64)            0         \n_________________________________________________________________\nmax_pooling1d_6 (MaxPooling1 (None, 8, 64)             0         \n_________________________________________________________________\nconv1d_24 (Conv1D)           (None, 8, 32)             16416     \n_________________________________________________________________\nconv1d_25 (Conv1D)           (None, 8, 32)             8224      \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 8, 32)             0         \n_________________________________________________________________\nconv1d_26 (Conv1D)           (None, 8, 16)             4112      \n_________________________________________________________________\nconv1d_27 (Conv1D)           (None, 8, 16)             2064      \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 8, 16)             64        \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 8, 16)             0         \n_________________________________________________________________\nmax_pooling1d_7 (MaxPooling1 (None, 2, 16)             0         \n_________________________________________________________________\nconv1d_28 (Conv1D)           (None, 2, 8)              1032      \n_________________________________________________________________\nconv1d_29 (Conv1D)           (None, 2, 8)              520       \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                170       \n=================================================================\nTotal params: 66,266\nTrainable params: 66,106\nNon-trainable params: 160\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpointer = ModelCheckpoint('music_genre.h5', monitor='val_acc', mode='max', verbose=2, save_best_only=True)\nhistory=model.fit(x_train, y_train, batch_size=64, epochs=250, validation_data=(x_test, y_test), callbacks=[checkpointer])","execution_count":56,"outputs":[{"output_type":"stream","text":"Epoch 1/250\n 1/12 [=>............................] - ETA: 0s - loss: 2.3352 - acc: 0.0625\nEpoch 00001: val_acc improved from -inf to 0.29600, saving model to music_genre.h5\n12/12 [==============================] - 0s 16ms/step - loss: 2.1842 - acc: 0.2320 - val_loss: 1.9457 - val_acc: 0.2960\nEpoch 2/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.9497 - acc: 0.3125\nEpoch 00002: val_acc improved from 0.29600 to 0.32000, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 1.8937 - acc: 0.3293 - val_loss: 1.7702 - val_acc: 0.3200\nEpoch 3/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.8667 - acc: 0.3594\nEpoch 00003: val_acc improved from 0.32000 to 0.35600, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 1.7557 - acc: 0.3560 - val_loss: 1.6368 - val_acc: 0.3560\nEpoch 4/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.6208 - acc: 0.4062\nEpoch 00004: val_acc improved from 0.35600 to 0.36800, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 1.6258 - acc: 0.4440 - val_loss: 1.5618 - val_acc: 0.3680\nEpoch 5/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.5914 - acc: 0.4688\nEpoch 00005: val_acc improved from 0.36800 to 0.43600, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 1.5482 - acc: 0.4613 - val_loss: 1.4671 - val_acc: 0.4360\nEpoch 6/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.5514 - acc: 0.4062\nEpoch 00006: val_acc improved from 0.43600 to 0.47200, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 1.4171 - acc: 0.4773 - val_loss: 1.3937 - val_acc: 0.4720\nEpoch 7/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.3897 - acc: 0.5156\nEpoch 00007: val_acc improved from 0.47200 to 0.48400, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 1.4070 - acc: 0.5013 - val_loss: 1.3553 - val_acc: 0.4840\nEpoch 8/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.3118 - acc: 0.5781\nEpoch 00008: val_acc improved from 0.48400 to 0.56000, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 1.3283 - acc: 0.5413 - val_loss: 1.2889 - val_acc: 0.5600\nEpoch 9/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.1592 - acc: 0.5781\nEpoch 00009: val_acc did not improve from 0.56000\n12/12 [==============================] - 0s 5ms/step - loss: 1.2589 - acc: 0.5533 - val_loss: 1.2538 - val_acc: 0.5560\nEpoch 10/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.1547 - acc: 0.6250\nEpoch 00010: val_acc did not improve from 0.56000\n12/12 [==============================] - 0s 4ms/step - loss: 1.2480 - acc: 0.5720 - val_loss: 1.2149 - val_acc: 0.5600\nEpoch 11/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.3179 - acc: 0.5625\nEpoch 00011: val_acc improved from 0.56000 to 0.56800, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 1.2180 - acc: 0.5627 - val_loss: 1.2056 - val_acc: 0.5680\nEpoch 12/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.3201 - acc: 0.5156\nEpoch 00012: val_acc improved from 0.56800 to 0.58800, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 1.1707 - acc: 0.5813 - val_loss: 1.1704 - val_acc: 0.5880\nEpoch 13/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.1682 - acc: 0.5156\nEpoch 00013: val_acc did not improve from 0.58800\n12/12 [==============================] - 0s 5ms/step - loss: 1.1476 - acc: 0.6067 - val_loss: 1.1453 - val_acc: 0.5640\nEpoch 14/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.1076 - acc: 0.6250\nEpoch 00014: val_acc did not improve from 0.58800\n12/12 [==============================] - 0s 4ms/step - loss: 1.0892 - acc: 0.6173 - val_loss: 1.1505 - val_acc: 0.5760\nEpoch 15/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.2572 - acc: 0.5469\nEpoch 00015: val_acc improved from 0.58800 to 0.59600, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 1.0464 - acc: 0.6547 - val_loss: 1.1259 - val_acc: 0.5960\nEpoch 16/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.9749 - acc: 0.6719\nEpoch 00016: val_acc did not improve from 0.59600\n12/12 [==============================] - 0s 5ms/step - loss: 1.0252 - acc: 0.6400 - val_loss: 1.1146 - val_acc: 0.5920\nEpoch 17/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.0885 - acc: 0.6094\nEpoch 00017: val_acc did not improve from 0.59600\n12/12 [==============================] - 0s 5ms/step - loss: 1.0549 - acc: 0.6400 - val_loss: 1.1202 - val_acc: 0.5920\nEpoch 18/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.8663 - acc: 0.7500\nEpoch 00018: val_acc improved from 0.59600 to 0.60800, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 0.9743 - acc: 0.6653 - val_loss: 1.0829 - val_acc: 0.6080\nEpoch 19/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.9384 - acc: 0.7344\nEpoch 00019: val_acc did not improve from 0.60800\n12/12 [==============================] - 0s 5ms/step - loss: 0.9500 - acc: 0.6720 - val_loss: 1.1271 - val_acc: 0.6000\nEpoch 20/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.1160 - acc: 0.6094\nEpoch 00020: val_acc improved from 0.60800 to 0.62400, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 0.9556 - acc: 0.6667 - val_loss: 1.0646 - val_acc: 0.6240\nEpoch 21/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.0336 - acc: 0.6719\nEpoch 00021: val_acc improved from 0.62400 to 0.63200, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 0.9014 - acc: 0.7000 - val_loss: 1.0673 - val_acc: 0.6320\nEpoch 22/250\n 1/12 [=>............................] - ETA: 0s - loss: 1.2849 - acc: 0.4844\nEpoch 00022: val_acc did not improve from 0.63200\n12/12 [==============================] - 0s 5ms/step - loss: 0.9219 - acc: 0.6800 - val_loss: 1.0826 - val_acc: 0.6160\nEpoch 23/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.7570 - acc: 0.7812\nEpoch 00023: val_acc did not improve from 0.63200\n12/12 [==============================] - 0s 5ms/step - loss: 0.8687 - acc: 0.7120 - val_loss: 1.0748 - val_acc: 0.6160\nEpoch 24/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6761 - acc: 0.7500\nEpoch 00024: val_acc improved from 0.63200 to 0.64000, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 0.8590 - acc: 0.7040 - val_loss: 1.0359 - val_acc: 0.6400\nEpoch 25/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.7794 - acc: 0.7656\nEpoch 00025: val_acc did not improve from 0.64000\n12/12 [==============================] - 0s 5ms/step - loss: 0.8169 - acc: 0.7133 - val_loss: 1.0641 - val_acc: 0.6120\nEpoch 26/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.9150 - acc: 0.7188\nEpoch 00026: val_acc did not improve from 0.64000\n12/12 [==============================] - 0s 5ms/step - loss: 0.8131 - acc: 0.7387 - val_loss: 1.0464 - val_acc: 0.6320\nEpoch 27/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6576 - acc: 0.7969\nEpoch 00027: val_acc did not improve from 0.64000\n12/12 [==============================] - 0s 5ms/step - loss: 0.8009 - acc: 0.7267 - val_loss: 1.0642 - val_acc: 0.6200\nEpoch 28/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.8169 - acc: 0.6406\nEpoch 00028: val_acc did not improve from 0.64000\n12/12 [==============================] - 0s 5ms/step - loss: 0.7774 - acc: 0.7453 - val_loss: 1.0194 - val_acc: 0.6320\nEpoch 29/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.9829 - acc: 0.6406\nEpoch 00029: val_acc did not improve from 0.64000\n12/12 [==============================] - 0s 5ms/step - loss: 0.7342 - acc: 0.7333 - val_loss: 1.0582 - val_acc: 0.6200\nEpoch 30/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6161 - acc: 0.7344\nEpoch 00030: val_acc did not improve from 0.64000\n12/12 [==============================] - 0s 5ms/step - loss: 0.7112 - acc: 0.7667 - val_loss: 1.0380 - val_acc: 0.6240\n","name":"stdout"},{"output_type":"stream","text":"Epoch 31/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6345 - acc: 0.7812\nEpoch 00031: val_acc did not improve from 0.64000\n12/12 [==============================] - 0s 5ms/step - loss: 0.7457 - acc: 0.7453 - val_loss: 1.0398 - val_acc: 0.6360\nEpoch 32/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.7697 - acc: 0.6875\nEpoch 00032: val_acc did not improve from 0.64000\n12/12 [==============================] - 0s 5ms/step - loss: 0.7191 - acc: 0.7480 - val_loss: 1.0759 - val_acc: 0.6120\nEpoch 33/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6741 - acc: 0.7656\nEpoch 00033: val_acc did not improve from 0.64000\n12/12 [==============================] - 0s 5ms/step - loss: 0.6932 - acc: 0.7640 - val_loss: 1.0306 - val_acc: 0.6320\nEpoch 34/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.5646 - acc: 0.8125\nEpoch 00034: val_acc improved from 0.64000 to 0.65200, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 0.6952 - acc: 0.7520 - val_loss: 1.0163 - val_acc: 0.6520\nEpoch 35/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.7540 - acc: 0.7969\nEpoch 00035: val_acc did not improve from 0.65200\n12/12 [==============================] - 0s 5ms/step - loss: 0.6592 - acc: 0.7787 - val_loss: 1.0667 - val_acc: 0.6480\nEpoch 36/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6057 - acc: 0.8125\nEpoch 00036: val_acc did not improve from 0.65200\n12/12 [==============================] - 0s 5ms/step - loss: 0.6446 - acc: 0.7853 - val_loss: 1.0735 - val_acc: 0.6360\nEpoch 37/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4524 - acc: 0.8906\nEpoch 00037: val_acc improved from 0.65200 to 0.66000, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 0.6133 - acc: 0.7893 - val_loss: 1.0246 - val_acc: 0.6600\nEpoch 38/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6741 - acc: 0.7656\nEpoch 00038: val_acc did not improve from 0.66000\n12/12 [==============================] - 0s 5ms/step - loss: 0.6392 - acc: 0.7693 - val_loss: 1.0628 - val_acc: 0.6120\nEpoch 39/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6233 - acc: 0.8125\nEpoch 00039: val_acc did not improve from 0.66000\n12/12 [==============================] - 0s 5ms/step - loss: 0.5886 - acc: 0.8040 - val_loss: 1.0076 - val_acc: 0.6520\nEpoch 40/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.8017 - acc: 0.7188\nEpoch 00040: val_acc did not improve from 0.66000\n12/12 [==============================] - 0s 5ms/step - loss: 0.6271 - acc: 0.7893 - val_loss: 1.0287 - val_acc: 0.6520\nEpoch 41/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6702 - acc: 0.7812\nEpoch 00041: val_acc did not improve from 0.66000\n12/12 [==============================] - 0s 5ms/step - loss: 0.5579 - acc: 0.8107 - val_loss: 1.0331 - val_acc: 0.6560\nEpoch 42/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.5341 - acc: 0.8125\nEpoch 00042: val_acc did not improve from 0.66000\n12/12 [==============================] - 0s 5ms/step - loss: 0.5806 - acc: 0.8000 - val_loss: 1.0732 - val_acc: 0.6400\nEpoch 43/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6621 - acc: 0.7656\nEpoch 00043: val_acc improved from 0.66000 to 0.66400, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 0.5518 - acc: 0.8200 - val_loss: 1.0135 - val_acc: 0.6640\nEpoch 44/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4605 - acc: 0.8281\nEpoch 00044: val_acc did not improve from 0.66400\n12/12 [==============================] - 0s 5ms/step - loss: 0.5346 - acc: 0.8053 - val_loss: 1.0548 - val_acc: 0.6640\nEpoch 45/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3755 - acc: 0.8594\nEpoch 00045: val_acc did not improve from 0.66400\n12/12 [==============================] - 0s 5ms/step - loss: 0.5082 - acc: 0.8267 - val_loss: 1.0326 - val_acc: 0.6600\nEpoch 46/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4315 - acc: 0.8750\nEpoch 00046: val_acc did not improve from 0.66400\n12/12 [==============================] - 0s 5ms/step - loss: 0.4953 - acc: 0.8280 - val_loss: 1.0676 - val_acc: 0.6640\nEpoch 47/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4576 - acc: 0.8906\nEpoch 00047: val_acc did not improve from 0.66400\n12/12 [==============================] - 0s 5ms/step - loss: 0.4952 - acc: 0.8360 - val_loss: 1.0681 - val_acc: 0.6480\nEpoch 48/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.5269 - acc: 0.7969\nEpoch 00048: val_acc did not improve from 0.66400\n12/12 [==============================] - 0s 5ms/step - loss: 0.4942 - acc: 0.8187 - val_loss: 1.0253 - val_acc: 0.6640\nEpoch 49/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3191 - acc: 0.9062\nEpoch 00049: val_acc did not improve from 0.66400\n12/12 [==============================] - 0s 5ms/step - loss: 0.4613 - acc: 0.8387 - val_loss: 1.1231 - val_acc: 0.6520\nEpoch 50/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4381 - acc: 0.8438\nEpoch 00050: val_acc improved from 0.66400 to 0.66800, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 0.5057 - acc: 0.8360 - val_loss: 1.0736 - val_acc: 0.6680\nEpoch 51/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3848 - acc: 0.8438\nEpoch 00051: val_acc improved from 0.66800 to 0.67200, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 0.4031 - acc: 0.8693 - val_loss: 1.0954 - val_acc: 0.6720\nEpoch 52/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.6087 - acc: 0.8125\nEpoch 00052: val_acc did not improve from 0.67200\n12/12 [==============================] - 0s 5ms/step - loss: 0.4464 - acc: 0.8560 - val_loss: 1.0770 - val_acc: 0.6640\nEpoch 53/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.5547 - acc: 0.8281\nEpoch 00053: val_acc did not improve from 0.67200\n12/12 [==============================] - 0s 5ms/step - loss: 0.4381 - acc: 0.8427 - val_loss: 1.1083 - val_acc: 0.6480\nEpoch 54/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4892 - acc: 0.8438\nEpoch 00054: val_acc did not improve from 0.67200\n12/12 [==============================] - 0s 5ms/step - loss: 0.4181 - acc: 0.8560 - val_loss: 1.1240 - val_acc: 0.6600\nEpoch 55/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3272 - acc: 0.8906\nEpoch 00055: val_acc improved from 0.67200 to 0.68400, saving model to music_genre.h5\n12/12 [==============================] - 0s 7ms/step - loss: 0.4079 - acc: 0.8653 - val_loss: 1.1370 - val_acc: 0.6840\nEpoch 56/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4086 - acc: 0.8750\nEpoch 00056: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.4093 - acc: 0.8680 - val_loss: 1.1425 - val_acc: 0.6480\nEpoch 57/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3942 - acc: 0.8594\nEpoch 00057: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3905 - acc: 0.8733 - val_loss: 1.1077 - val_acc: 0.6760\nEpoch 58/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4349 - acc: 0.8750\nEpoch 00058: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3786 - acc: 0.8773 - val_loss: 1.1106 - val_acc: 0.6680\nEpoch 59/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4160 - acc: 0.8281\nEpoch 00059: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3978 - acc: 0.8627 - val_loss: 1.1457 - val_acc: 0.6720\nEpoch 60/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4127 - acc: 0.8594\nEpoch 00060: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.4085 - acc: 0.8640 - val_loss: 1.1659 - val_acc: 0.6520\nEpoch 61/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3924 - acc: 0.8281\nEpoch 00061: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 4ms/step - loss: 0.3642 - acc: 0.8787 - val_loss: 1.1403 - val_acc: 0.6600\n","name":"stdout"},{"output_type":"stream","text":"Epoch 62/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3470 - acc: 0.9062\nEpoch 00062: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 4ms/step - loss: 0.3637 - acc: 0.8787 - val_loss: 1.1586 - val_acc: 0.6680\nEpoch 63/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4049 - acc: 0.8906\nEpoch 00063: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.4036 - acc: 0.8720 - val_loss: 1.1352 - val_acc: 0.6640\nEpoch 64/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1874 - acc: 0.9531\nEpoch 00064: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3576 - acc: 0.8880 - val_loss: 1.1687 - val_acc: 0.6640\nEpoch 65/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.5317 - acc: 0.8281\nEpoch 00065: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3997 - acc: 0.8720 - val_loss: 1.1694 - val_acc: 0.6760\nEpoch 66/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3678 - acc: 0.8125\nEpoch 00066: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3581 - acc: 0.8800 - val_loss: 1.1623 - val_acc: 0.6640\nEpoch 67/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3864 - acc: 0.8281\nEpoch 00067: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 4ms/step - loss: 0.3435 - acc: 0.8733 - val_loss: 1.1848 - val_acc: 0.6640\nEpoch 68/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.5178 - acc: 0.8438\nEpoch 00068: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3453 - acc: 0.8840 - val_loss: 1.1310 - val_acc: 0.6840\nEpoch 69/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2379 - acc: 0.8906\nEpoch 00069: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3407 - acc: 0.8853 - val_loss: 1.1473 - val_acc: 0.6760\nEpoch 70/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2824 - acc: 0.9219\nEpoch 00070: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3234 - acc: 0.9040 - val_loss: 1.1739 - val_acc: 0.6680\nEpoch 71/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4522 - acc: 0.8438\nEpoch 00071: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3341 - acc: 0.8960 - val_loss: 1.1585 - val_acc: 0.6760\nEpoch 72/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2413 - acc: 0.9062\nEpoch 00072: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.3172 - acc: 0.8960 - val_loss: 1.2489 - val_acc: 0.6480\nEpoch 73/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2372 - acc: 0.8906\nEpoch 00073: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.2639 - acc: 0.9080 - val_loss: 1.2187 - val_acc: 0.6640\nEpoch 74/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.4147 - acc: 0.8438\nEpoch 00074: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.2817 - acc: 0.9000 - val_loss: 1.1774 - val_acc: 0.6840\nEpoch 75/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3282 - acc: 0.9062\nEpoch 00075: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.2824 - acc: 0.9040 - val_loss: 1.2494 - val_acc: 0.6560\nEpoch 76/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2948 - acc: 0.9062\nEpoch 00076: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.2806 - acc: 0.9107 - val_loss: 1.2760 - val_acc: 0.6640\nEpoch 77/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2455 - acc: 0.9531\nEpoch 00077: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.2702 - acc: 0.9187 - val_loss: 1.2370 - val_acc: 0.6800\nEpoch 78/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1982 - acc: 0.9531\nEpoch 00078: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 4ms/step - loss: 0.2913 - acc: 0.9080 - val_loss: 1.1942 - val_acc: 0.6800\nEpoch 79/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2321 - acc: 0.9375\nEpoch 00079: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.2965 - acc: 0.8987 - val_loss: 1.2108 - val_acc: 0.6800\nEpoch 80/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3952 - acc: 0.8594\nEpoch 00080: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.2716 - acc: 0.9040 - val_loss: 1.2231 - val_acc: 0.6680\nEpoch 81/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2687 - acc: 0.9062\nEpoch 00081: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.2778 - acc: 0.9080 - val_loss: 1.2399 - val_acc: 0.6640\nEpoch 82/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2732 - acc: 0.9375\nEpoch 00082: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 5ms/step - loss: 0.2801 - acc: 0.9133 - val_loss: 1.2539 - val_acc: 0.6680\nEpoch 83/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2331 - acc: 0.9531\nEpoch 00083: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 4ms/step - loss: 0.2712 - acc: 0.9147 - val_loss: 1.2637 - val_acc: 0.6640\nEpoch 84/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2025 - acc: 0.9375\nEpoch 00084: val_acc did not improve from 0.68400\n12/12 [==============================] - 0s 4ms/step - loss: 0.2450 - acc: 0.9267 - val_loss: 1.3060 - val_acc: 0.6520\nEpoch 85/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3574 - acc: 0.9219\nEpoch 00085: val_acc improved from 0.68400 to 0.68800, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 0.2432 - acc: 0.9147 - val_loss: 1.2540 - val_acc: 0.6880\nEpoch 86/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2263 - acc: 0.9375\nEpoch 00086: val_acc improved from 0.68800 to 0.69200, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 0.2521 - acc: 0.9267 - val_loss: 1.2537 - val_acc: 0.6920\nEpoch 87/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2937 - acc: 0.9219\nEpoch 00087: val_acc did not improve from 0.69200\n12/12 [==============================] - 0s 5ms/step - loss: 0.2575 - acc: 0.9227 - val_loss: 1.2909 - val_acc: 0.6840\nEpoch 88/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2190 - acc: 0.9062\nEpoch 00088: val_acc did not improve from 0.69200\n12/12 [==============================] - 0s 4ms/step - loss: 0.2425 - acc: 0.9133 - val_loss: 1.2855 - val_acc: 0.6680\nEpoch 89/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2603 - acc: 0.8906\nEpoch 00089: val_acc did not improve from 0.69200\n12/12 [==============================] - 0s 4ms/step - loss: 0.2012 - acc: 0.9387 - val_loss: 1.3011 - val_acc: 0.6720\nEpoch 90/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1687 - acc: 0.9531\nEpoch 00090: val_acc did not improve from 0.69200\n12/12 [==============================] - 0s 4ms/step - loss: 0.2059 - acc: 0.9333 - val_loss: 1.2703 - val_acc: 0.6760\nEpoch 91/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3230 - acc: 0.8906\nEpoch 00091: val_acc improved from 0.69200 to 0.69600, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 0.2431 - acc: 0.9253 - val_loss: 1.2749 - val_acc: 0.6960\nEpoch 92/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2124 - acc: 0.9062\nEpoch 00092: val_acc improved from 0.69600 to 0.70000, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 0.2149 - acc: 0.9280 - val_loss: 1.2911 - val_acc: 0.7000\n","name":"stdout"},{"output_type":"stream","text":"Epoch 93/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1600 - acc: 0.9531\nEpoch 00093: val_acc did not improve from 0.70000\n12/12 [==============================] - 0s 4ms/step - loss: 0.2169 - acc: 0.9360 - val_loss: 1.3151 - val_acc: 0.6920\nEpoch 94/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2337 - acc: 0.9375\nEpoch 00094: val_acc did not improve from 0.70000\n12/12 [==============================] - 0s 4ms/step - loss: 0.2107 - acc: 0.9360 - val_loss: 1.2777 - val_acc: 0.6880\nEpoch 95/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3422 - acc: 0.8750\nEpoch 00095: val_acc did not improve from 0.70000\n12/12 [==============================] - 0s 5ms/step - loss: 0.2218 - acc: 0.9253 - val_loss: 1.2167 - val_acc: 0.6880\nEpoch 96/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2159 - acc: 0.8906\nEpoch 00096: val_acc improved from 0.70000 to 0.70800, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 0.2277 - acc: 0.9227 - val_loss: 1.1993 - val_acc: 0.7080\nEpoch 97/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2838 - acc: 0.9219\nEpoch 00097: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 5ms/step - loss: 0.2023 - acc: 0.9373 - val_loss: 1.3189 - val_acc: 0.6600\nEpoch 98/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1945 - acc: 0.9688\nEpoch 00098: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.2044 - acc: 0.9400 - val_loss: 1.3506 - val_acc: 0.6800\nEpoch 99/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1489 - acc: 0.9531\nEpoch 00099: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.2022 - acc: 0.9400 - val_loss: 1.3293 - val_acc: 0.6720\nEpoch 100/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2205 - acc: 0.9219\nEpoch 00100: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1881 - acc: 0.9427 - val_loss: 1.3416 - val_acc: 0.6640\nEpoch 101/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1918 - acc: 0.9219\nEpoch 00101: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.2100 - acc: 0.9387 - val_loss: 1.3199 - val_acc: 0.6720\nEpoch 102/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1551 - acc: 0.9688\nEpoch 00102: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1898 - acc: 0.9360 - val_loss: 1.3673 - val_acc: 0.6720\nEpoch 103/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2213 - acc: 0.9375\nEpoch 00103: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1775 - acc: 0.9413 - val_loss: 1.3743 - val_acc: 0.6640\nEpoch 104/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1360 - acc: 0.9531\nEpoch 00104: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.2079 - acc: 0.9307 - val_loss: 1.3461 - val_acc: 0.6880\nEpoch 105/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3054 - acc: 0.8750\nEpoch 00105: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.2076 - acc: 0.9293 - val_loss: 1.2625 - val_acc: 0.6840\nEpoch 106/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.3243 - acc: 0.8906\nEpoch 00106: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1811 - acc: 0.9400 - val_loss: 1.3151 - val_acc: 0.6760\nEpoch 107/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1737 - acc: 0.9688\nEpoch 00107: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 5ms/step - loss: 0.1645 - acc: 0.9547 - val_loss: 1.3746 - val_acc: 0.6760\nEpoch 108/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1448 - acc: 0.9531\nEpoch 00108: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1855 - acc: 0.9440 - val_loss: 1.3121 - val_acc: 0.6760\nEpoch 109/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1287 - acc: 0.9375\nEpoch 00109: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1828 - acc: 0.9427 - val_loss: 1.3724 - val_acc: 0.6800\nEpoch 110/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2815 - acc: 0.9375\nEpoch 00110: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1873 - acc: 0.9467 - val_loss: 1.3426 - val_acc: 0.6960\nEpoch 111/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1630 - acc: 0.9531\nEpoch 00111: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1850 - acc: 0.9467 - val_loss: 1.3727 - val_acc: 0.6880\nEpoch 112/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2933 - acc: 0.8906\nEpoch 00112: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1461 - acc: 0.9573 - val_loss: 1.4261 - val_acc: 0.6720\nEpoch 113/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1697 - acc: 0.9531\nEpoch 00113: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1437 - acc: 0.9573 - val_loss: 1.4094 - val_acc: 0.6640\nEpoch 114/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1667 - acc: 0.9219\nEpoch 00114: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 5ms/step - loss: 0.1636 - acc: 0.9427 - val_loss: 1.4281 - val_acc: 0.6680\nEpoch 115/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1714 - acc: 0.9375\nEpoch 00115: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1338 - acc: 0.9560 - val_loss: 1.4457 - val_acc: 0.6720\nEpoch 116/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2228 - acc: 0.9375\nEpoch 00116: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1482 - acc: 0.9627 - val_loss: 1.3777 - val_acc: 0.6920\nEpoch 117/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1792 - acc: 0.9375\nEpoch 00117: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1436 - acc: 0.9600 - val_loss: 1.4248 - val_acc: 0.6840\nEpoch 118/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1169 - acc: 0.9531\nEpoch 00118: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 4ms/step - loss: 0.1527 - acc: 0.9493 - val_loss: 1.4498 - val_acc: 0.6920\nEpoch 119/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1561 - acc: 0.9531\nEpoch 00119: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 5ms/step - loss: 0.1534 - acc: 0.9587 - val_loss: 1.5018 - val_acc: 0.6720\nEpoch 120/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1808 - acc: 0.9531\nEpoch 00120: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 5ms/step - loss: 0.1763 - acc: 0.9440 - val_loss: 1.4608 - val_acc: 0.6680\nEpoch 121/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1557 - acc: 0.9375\nEpoch 00121: val_acc did not improve from 0.70800\n12/12 [==============================] - 0s 5ms/step - loss: 0.1727 - acc: 0.9427 - val_loss: 1.4904 - val_acc: 0.6720\nEpoch 122/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2410 - acc: 0.9062\nEpoch 00122: val_acc improved from 0.70800 to 0.71200, saving model to music_genre.h5\n12/12 [==============================] - 0s 6ms/step - loss: 0.1554 - acc: 0.9493 - val_loss: 1.4557 - val_acc: 0.7120\nEpoch 123/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1997 - acc: 0.9219\nEpoch 00123: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.1948 - acc: 0.9360 - val_loss: 1.4692 - val_acc: 0.6720\nEpoch 124/250\n","name":"stdout"},{"output_type":"stream","text":" 1/12 [=>............................] - ETA: 0s - loss: 0.0897 - acc: 0.9688\nEpoch 00124: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.1682 - acc: 0.9587 - val_loss: 1.4059 - val_acc: 0.6880\nEpoch 125/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2396 - acc: 0.9219\nEpoch 00125: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1975 - acc: 0.9387 - val_loss: 1.4025 - val_acc: 0.6920\nEpoch 126/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1508 - acc: 0.9375\nEpoch 00126: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1388 - acc: 0.9520 - val_loss: 1.3811 - val_acc: 0.6800\nEpoch 127/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1061 - acc: 0.9844\nEpoch 00127: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.1128 - acc: 0.9747 - val_loss: 1.3689 - val_acc: 0.6920\nEpoch 128/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1292 - acc: 0.9688\nEpoch 00128: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.1258 - acc: 0.9640 - val_loss: 1.4635 - val_acc: 0.6960\nEpoch 129/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1524 - acc: 0.9844\nEpoch 00129: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1355 - acc: 0.9627 - val_loss: 1.4485 - val_acc: 0.6880\nEpoch 130/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0684 - acc: 1.0000\nEpoch 00130: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1232 - acc: 0.9720 - val_loss: 1.4362 - val_acc: 0.7080\nEpoch 131/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0585 - acc: 0.9844\nEpoch 00131: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1121 - acc: 0.9627 - val_loss: 1.4349 - val_acc: 0.6960\nEpoch 132/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2279 - acc: 0.9062\nEpoch 00132: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.1261 - acc: 0.9653 - val_loss: 1.4443 - val_acc: 0.6960\nEpoch 133/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1124 - acc: 0.9688\nEpoch 00133: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1033 - acc: 0.9653 - val_loss: 1.4849 - val_acc: 0.7120\nEpoch 134/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0884 - acc: 0.9844\nEpoch 00134: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1538 - acc: 0.9547 - val_loss: 1.5357 - val_acc: 0.6800\nEpoch 135/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0657 - acc: 1.0000\nEpoch 00135: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1049 - acc: 0.9760 - val_loss: 1.5239 - val_acc: 0.6800\nEpoch 136/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1049 - acc: 0.9844\nEpoch 00136: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1285 - acc: 0.9627 - val_loss: 1.4620 - val_acc: 0.7040\nEpoch 137/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0544 - acc: 0.9844\nEpoch 00137: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.1190 - acc: 0.9587 - val_loss: 1.4859 - val_acc: 0.7080\nEpoch 138/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1192 - acc: 0.9531\nEpoch 00138: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1415 - acc: 0.9627 - val_loss: 1.5139 - val_acc: 0.6800\nEpoch 139/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0919 - acc: 0.9844\nEpoch 00139: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1085 - acc: 0.9627 - val_loss: 1.4791 - val_acc: 0.6640\nEpoch 140/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1027 - acc: 0.9531\nEpoch 00140: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1281 - acc: 0.9520 - val_loss: 1.5316 - val_acc: 0.6720\nEpoch 141/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1790 - acc: 0.9531\nEpoch 00141: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1225 - acc: 0.9720 - val_loss: 1.6077 - val_acc: 0.6720\nEpoch 142/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1002 - acc: 0.9688\nEpoch 00142: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1065 - acc: 0.9707 - val_loss: 1.5786 - val_acc: 0.6800\nEpoch 143/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0996 - acc: 0.9688\nEpoch 00143: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1107 - acc: 0.9640 - val_loss: 1.4828 - val_acc: 0.6960\nEpoch 144/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1181 - acc: 0.9688\nEpoch 00144: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0860 - acc: 0.9773 - val_loss: 1.5047 - val_acc: 0.7080\nEpoch 145/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0976 - acc: 0.9844\nEpoch 00145: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1469 - acc: 0.9533 - val_loss: 1.5586 - val_acc: 0.6920\nEpoch 146/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1587 - acc: 0.9375\nEpoch 00146: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0889 - acc: 0.9733 - val_loss: 1.5609 - val_acc: 0.7000\nEpoch 147/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1302 - acc: 0.9531\nEpoch 00147: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.1070 - acc: 0.9693 - val_loss: 1.5314 - val_acc: 0.6960\nEpoch 148/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0318 - acc: 1.0000\nEpoch 00148: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0978 - acc: 0.9653 - val_loss: 1.5154 - val_acc: 0.6960\nEpoch 149/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0598 - acc: 0.9844\nEpoch 00149: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0927 - acc: 0.9747 - val_loss: 1.5692 - val_acc: 0.6960\nEpoch 150/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0306 - acc: 1.0000\nEpoch 00150: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1010 - acc: 0.9653 - val_loss: 1.5881 - val_acc: 0.7040\nEpoch 151/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0728 - acc: 0.9844\nEpoch 00151: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0917 - acc: 0.9680 - val_loss: 1.5736 - val_acc: 0.6880\nEpoch 152/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0683 - acc: 0.9844\nEpoch 00152: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0988 - acc: 0.9680 - val_loss: 1.6285 - val_acc: 0.6800\nEpoch 153/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0572 - acc: 1.0000\nEpoch 00153: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0949 - acc: 0.9720 - val_loss: 1.6101 - val_acc: 0.6680\nEpoch 154/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2448 - acc: 0.9531\nEpoch 00154: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1255 - acc: 0.9707 - val_loss: 1.6113 - val_acc: 0.6840\nEpoch 155/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0497 - acc: 0.9844\nEpoch 00155: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0895 - acc: 0.9707 - val_loss: 1.6107 - val_acc: 0.6880\n","name":"stdout"},{"output_type":"stream","text":"Epoch 156/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0640 - acc: 0.9688\nEpoch 00156: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0984 - acc: 0.9667 - val_loss: 1.5330 - val_acc: 0.6840\nEpoch 157/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0768 - acc: 0.9531\nEpoch 00157: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0998 - acc: 0.9667 - val_loss: 1.6288 - val_acc: 0.6720\nEpoch 158/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0998 - acc: 0.9531\nEpoch 00158: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1019 - acc: 0.9707 - val_loss: 1.7216 - val_acc: 0.6880\nEpoch 159/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1230 - acc: 0.9688\nEpoch 00159: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0877 - acc: 0.9733 - val_loss: 1.7062 - val_acc: 0.6800\nEpoch 160/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1737 - acc: 0.9531\nEpoch 00160: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1099 - acc: 0.9640 - val_loss: 1.7086 - val_acc: 0.6680\nEpoch 161/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0823 - acc: 0.9688\nEpoch 00161: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1048 - acc: 0.9627 - val_loss: 1.7852 - val_acc: 0.6760\nEpoch 162/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1352 - acc: 0.9531\nEpoch 00162: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1213 - acc: 0.9640 - val_loss: 1.7705 - val_acc: 0.6680\nEpoch 163/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0719 - acc: 0.9531\nEpoch 00163: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0935 - acc: 0.9707 - val_loss: 1.6345 - val_acc: 0.6800\nEpoch 164/250\n12/12 [==============================] - ETA: 0s - loss: 0.0868 - acc: 0.9773\nEpoch 00164: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 7ms/step - loss: 0.0868 - acc: 0.9773 - val_loss: 1.6319 - val_acc: 0.6800\nEpoch 165/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0830 - acc: 0.9688\nEpoch 00165: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.1071 - acc: 0.9653 - val_loss: 1.7185 - val_acc: 0.6680\nEpoch 166/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0855 - acc: 0.9844\nEpoch 00166: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0735 - acc: 0.9800 - val_loss: 1.7733 - val_acc: 0.6640\nEpoch 167/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0705 - acc: 0.9844\nEpoch 00167: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0695 - acc: 0.9853 - val_loss: 1.7315 - val_acc: 0.6960\nEpoch 168/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0407 - acc: 1.0000\nEpoch 00168: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0742 - acc: 0.9760 - val_loss: 1.7201 - val_acc: 0.6720\nEpoch 169/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0702 - acc: 0.9844\nEpoch 00169: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0722 - acc: 0.9813 - val_loss: 1.7297 - val_acc: 0.6760\nEpoch 170/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0873 - acc: 0.9688\nEpoch 00170: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0951 - acc: 0.9693 - val_loss: 1.6273 - val_acc: 0.6840\nEpoch 171/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2454 - acc: 0.9688\nEpoch 00171: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1165 - acc: 0.9707 - val_loss: 1.6285 - val_acc: 0.6760\nEpoch 172/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0619 - acc: 0.9688\nEpoch 00172: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.1030 - acc: 0.9680 - val_loss: 1.7013 - val_acc: 0.6720\nEpoch 173/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0523 - acc: 0.9844\nEpoch 00173: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1103 - acc: 0.9680 - val_loss: 1.6411 - val_acc: 0.6720\nEpoch 174/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0728 - acc: 0.9688\nEpoch 00174: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1091 - acc: 0.9653 - val_loss: 1.6291 - val_acc: 0.6760\nEpoch 175/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0336 - acc: 1.0000\nEpoch 00175: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0913 - acc: 0.9707 - val_loss: 1.5832 - val_acc: 0.6880\nEpoch 176/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0755 - acc: 0.9688\nEpoch 00176: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0983 - acc: 0.9640 - val_loss: 1.5870 - val_acc: 0.6920\nEpoch 177/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0633 - acc: 0.9844\nEpoch 00177: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0812 - acc: 0.9733 - val_loss: 1.6175 - val_acc: 0.6720\nEpoch 178/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0809 - acc: 0.9844\nEpoch 00178: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0756 - acc: 0.9773 - val_loss: 1.6954 - val_acc: 0.6720\nEpoch 179/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0727 - acc: 0.9688\nEpoch 00179: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0597 - acc: 0.9840 - val_loss: 1.7553 - val_acc: 0.6880\nEpoch 180/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0643 - acc: 1.0000\nEpoch 00180: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0763 - acc: 0.9773 - val_loss: 1.7710 - val_acc: 0.6760\nEpoch 181/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1254 - acc: 0.9688\nEpoch 00181: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0786 - acc: 0.9813 - val_loss: 1.7517 - val_acc: 0.6760\nEpoch 182/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0821 - acc: 0.9531\nEpoch 00182: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0812 - acc: 0.9760 - val_loss: 1.7349 - val_acc: 0.6760\nEpoch 183/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0680 - acc: 0.9844\nEpoch 00183: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0813 - acc: 0.9720 - val_loss: 1.7026 - val_acc: 0.6760\nEpoch 184/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0620 - acc: 0.9844\nEpoch 00184: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0640 - acc: 0.9867 - val_loss: 1.8016 - val_acc: 0.6600\nEpoch 185/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1011 - acc: 0.9688\nEpoch 00185: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0786 - acc: 0.9760 - val_loss: 1.8114 - val_acc: 0.6840\nEpoch 186/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0297 - acc: 1.0000\nEpoch 00186: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0603 - acc: 0.9840 - val_loss: 1.7685 - val_acc: 0.6800\nEpoch 187/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1244 - acc: 0.9531\nEpoch 00187: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0925 - acc: 0.9787 - val_loss: 1.7175 - val_acc: 0.6960\n","name":"stdout"},{"output_type":"stream","text":"Epoch 188/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1845 - acc: 0.9375\nEpoch 00188: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0945 - acc: 0.9733 - val_loss: 1.7669 - val_acc: 0.6920\nEpoch 189/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1716 - acc: 0.9375\nEpoch 00189: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0929 - acc: 0.9680 - val_loss: 1.6630 - val_acc: 0.6800\nEpoch 190/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0713 - acc: 0.9688\nEpoch 00190: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0977 - acc: 0.9680 - val_loss: 1.6940 - val_acc: 0.6760\nEpoch 191/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0525 - acc: 0.9844\nEpoch 00191: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0644 - acc: 0.9867 - val_loss: 1.7995 - val_acc: 0.6880\nEpoch 192/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0915 - acc: 0.9688\nEpoch 00192: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0726 - acc: 0.9800 - val_loss: 1.7511 - val_acc: 0.6920\nEpoch 193/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0966 - acc: 0.9688\nEpoch 00193: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0799 - acc: 0.9747 - val_loss: 1.7250 - val_acc: 0.6840\nEpoch 194/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0982 - acc: 0.9688\nEpoch 00194: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0690 - acc: 0.9813 - val_loss: 1.7354 - val_acc: 0.6840\nEpoch 195/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0519 - acc: 1.0000\nEpoch 00195: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0780 - acc: 0.9827 - val_loss: 1.6960 - val_acc: 0.6880\nEpoch 196/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1407 - acc: 0.9531\nEpoch 00196: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0751 - acc: 0.9800 - val_loss: 1.6916 - val_acc: 0.7080\nEpoch 197/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0983 - acc: 0.9531\nEpoch 00197: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0638 - acc: 0.9813 - val_loss: 1.7933 - val_acc: 0.6960\nEpoch 198/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0707 - acc: 0.9844\nEpoch 00198: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0797 - acc: 0.9707 - val_loss: 1.8544 - val_acc: 0.6760\nEpoch 199/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0543 - acc: 0.9844\nEpoch 00199: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0473 - acc: 0.9880 - val_loss: 1.7330 - val_acc: 0.6880\nEpoch 200/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0712 - acc: 0.9844\nEpoch 00200: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0754 - acc: 0.9773 - val_loss: 1.7166 - val_acc: 0.7040\nEpoch 201/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0670 - acc: 0.9688\nEpoch 00201: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0627 - acc: 0.9827 - val_loss: 1.8297 - val_acc: 0.6880\nEpoch 202/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0563 - acc: 0.9844\nEpoch 00202: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0743 - acc: 0.9760 - val_loss: 1.8726 - val_acc: 0.6800\nEpoch 203/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0582 - acc: 0.9844\nEpoch 00203: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0711 - acc: 0.9733 - val_loss: 1.9097 - val_acc: 0.6880\nEpoch 204/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0373 - acc: 0.9844\nEpoch 00204: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0825 - acc: 0.9747 - val_loss: 1.8637 - val_acc: 0.6920\nEpoch 205/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0840 - acc: 0.9531\nEpoch 00205: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.1058 - acc: 0.9707 - val_loss: 1.8125 - val_acc: 0.6920\nEpoch 206/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0400 - acc: 1.0000\nEpoch 00206: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0483 - acc: 0.9853 - val_loss: 1.7706 - val_acc: 0.6800\nEpoch 207/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1115 - acc: 0.9844\nEpoch 00207: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0782 - acc: 0.9760 - val_loss: 1.7975 - val_acc: 0.6840\nEpoch 208/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1631 - acc: 0.9844\nEpoch 00208: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0829 - acc: 0.9827 - val_loss: 1.8001 - val_acc: 0.6880\nEpoch 209/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0069 - acc: 1.0000\nEpoch 00209: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0826 - acc: 0.9800 - val_loss: 1.7914 - val_acc: 0.6920\nEpoch 210/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0905 - acc: 0.9688\nEpoch 00210: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0937 - acc: 0.9693 - val_loss: 1.7435 - val_acc: 0.6840\nEpoch 211/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.2343 - acc: 0.9375\nEpoch 00211: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0547 - acc: 0.9920 - val_loss: 1.8813 - val_acc: 0.6760\nEpoch 212/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1604 - acc: 0.9688\nEpoch 00212: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0959 - acc: 0.9787 - val_loss: 1.8844 - val_acc: 0.6680\nEpoch 213/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0306 - acc: 1.0000\nEpoch 00213: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.1009 - acc: 0.9707 - val_loss: 1.8867 - val_acc: 0.6800\nEpoch 214/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0427 - acc: 1.0000\nEpoch 00214: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0657 - acc: 0.9827 - val_loss: 1.7209 - val_acc: 0.7000\nEpoch 215/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0544 - acc: 0.9844\nEpoch 00215: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0713 - acc: 0.9747 - val_loss: 1.7488 - val_acc: 0.6840\nEpoch 216/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0239 - acc: 1.0000\nEpoch 00216: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0598 - acc: 0.9840 - val_loss: 1.7972 - val_acc: 0.6960\nEpoch 217/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0318 - acc: 0.9844\nEpoch 00217: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0507 - acc: 0.9853 - val_loss: 1.9275 - val_acc: 0.6760\nEpoch 218/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0322 - acc: 1.0000\nEpoch 00218: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0661 - acc: 0.9760 - val_loss: 1.9361 - val_acc: 0.6720\nEpoch 219/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0402 - acc: 0.9844\nEpoch 00219: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0563 - acc: 0.9747 - val_loss: 1.8648 - val_acc: 0.6760\n","name":"stdout"},{"output_type":"stream","text":"Epoch 220/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0418 - acc: 0.9844\nEpoch 00220: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0739 - acc: 0.9760 - val_loss: 1.7827 - val_acc: 0.6920\nEpoch 221/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0811 - acc: 0.9688\nEpoch 00221: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0534 - acc: 0.9840 - val_loss: 1.7558 - val_acc: 0.6920\nEpoch 222/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0459 - acc: 1.0000\nEpoch 00222: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0714 - acc: 0.9813 - val_loss: 1.8098 - val_acc: 0.6960\nEpoch 223/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0233 - acc: 1.0000\nEpoch 00223: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0504 - acc: 0.9840 - val_loss: 1.8433 - val_acc: 0.6880\nEpoch 224/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0193 - acc: 1.0000\nEpoch 00224: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0486 - acc: 0.9867 - val_loss: 1.8601 - val_acc: 0.6840\nEpoch 225/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0657 - acc: 0.9688\nEpoch 00225: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0755 - acc: 0.9787 - val_loss: 1.8663 - val_acc: 0.6840\nEpoch 226/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0500 - acc: 0.9844\nEpoch 00226: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0800 - acc: 0.9733 - val_loss: 1.8897 - val_acc: 0.6840\nEpoch 227/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1269 - acc: 0.9844\nEpoch 00227: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0570 - acc: 0.9853 - val_loss: 1.8496 - val_acc: 0.6920\nEpoch 228/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0934 - acc: 0.9531\nEpoch 00228: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0676 - acc: 0.9773 - val_loss: 1.8357 - val_acc: 0.6880\nEpoch 229/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0526 - acc: 0.9688\nEpoch 00229: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0550 - acc: 0.9827 - val_loss: 1.9044 - val_acc: 0.6720\nEpoch 230/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0905 - acc: 0.9844\nEpoch 00230: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0511 - acc: 0.9867 - val_loss: 1.8767 - val_acc: 0.6760\nEpoch 231/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1301 - acc: 0.9531\nEpoch 00231: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0783 - acc: 0.9787 - val_loss: 1.8901 - val_acc: 0.6760\nEpoch 232/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1036 - acc: 0.9531\nEpoch 00232: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0728 - acc: 0.9800 - val_loss: 1.8201 - val_acc: 0.6760\nEpoch 233/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1002 - acc: 0.9688\nEpoch 00233: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0610 - acc: 0.9840 - val_loss: 1.7820 - val_acc: 0.6840\nEpoch 234/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0283 - acc: 1.0000\nEpoch 00234: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0605 - acc: 0.9840 - val_loss: 1.7495 - val_acc: 0.6800\nEpoch 235/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0875 - acc: 0.9688\nEpoch 00235: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0928 - acc: 0.9667 - val_loss: 1.7233 - val_acc: 0.6840\nEpoch 236/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0724 - acc: 0.9844\nEpoch 00236: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 5ms/step - loss: 0.0488 - acc: 0.9880 - val_loss: 1.6902 - val_acc: 0.6800\nEpoch 237/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0770 - acc: 0.9531\nEpoch 00237: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0558 - acc: 0.9827 - val_loss: 1.7312 - val_acc: 0.6840\nEpoch 238/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0417 - acc: 0.9844\nEpoch 00238: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0604 - acc: 0.9787 - val_loss: 1.7557 - val_acc: 0.7000\nEpoch 239/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0212 - acc: 1.0000\nEpoch 00239: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0620 - acc: 0.9840 - val_loss: 1.8096 - val_acc: 0.6960\nEpoch 240/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1086 - acc: 0.9688\nEpoch 00240: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0628 - acc: 0.9787 - val_loss: 1.9324 - val_acc: 0.6800\nEpoch 241/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0349 - acc: 0.9844\nEpoch 00241: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0861 - acc: 0.9747 - val_loss: 1.9087 - val_acc: 0.6680\nEpoch 242/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0751 - acc: 0.9844\nEpoch 00242: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0540 - acc: 0.9853 - val_loss: 1.7533 - val_acc: 0.6760\nEpoch 243/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0780 - acc: 0.9688\nEpoch 00243: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0609 - acc: 0.9773 - val_loss: 1.7835 - val_acc: 0.6960\nEpoch 244/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.1275 - acc: 0.9531\nEpoch 00244: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0784 - acc: 0.9747 - val_loss: 1.8386 - val_acc: 0.7000\nEpoch 245/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0159 - acc: 1.0000\nEpoch 00245: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0481 - acc: 0.9813 - val_loss: 1.7909 - val_acc: 0.7000\nEpoch 246/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0286 - acc: 1.0000\nEpoch 00246: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0446 - acc: 0.9907 - val_loss: 1.8138 - val_acc: 0.6920\nEpoch 247/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0309 - acc: 0.9844\nEpoch 00247: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0355 - acc: 0.9907 - val_loss: 1.8809 - val_acc: 0.6840\nEpoch 248/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0357 - acc: 1.0000\nEpoch 00248: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0468 - acc: 0.9813 - val_loss: 1.9013 - val_acc: 0.6800\nEpoch 249/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0301 - acc: 0.9844\nEpoch 00249: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0332 - acc: 0.9907 - val_loss: 1.9464 - val_acc: 0.6920\nEpoch 250/250\n 1/12 [=>............................] - ETA: 0s - loss: 0.0564 - acc: 0.9844\nEpoch 00250: val_acc did not improve from 0.71200\n12/12 [==============================] - 0s 4ms/step - loss: 0.0389 - acc: 0.9893 - val_loss: 1.9601 - val_acc: 0.6920\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"present_model = tf.keras.models.load_model('music_genre.h5')\npresent_model.summary()","execution_count":58,"outputs":[{"output_type":"stream","text":"Model: \"sequential_14\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_43 (Dense)             (None, 512)               13824     \n_________________________________________________________________\ndropout_36 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_44 (Dense)             (None, 256)               131328    \n_________________________________________________________________\ndropout_37 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_45 (Dense)             (None, 64)                16448     \n_________________________________________________________________\ndropout_38 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_46 (Dense)             (None, 10)                650       \n=================================================================\nTotal params: 162,250\nTrainable params: 162,250\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , present_model.evaluate(x_test,y_test)[1]*100 , \"%\")","execution_count":59,"outputs":[{"output_type":"stream","text":"8/8 [==============================] - 0s 1ms/step - loss: 2.0749 - acc: 0.7120\nAccuracy of our model on test data :  71.20000123977661 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the training artifacts\nimport matplotlib.pyplot as plt\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train_acc','val_acc'], loc = 'upper right')\nplt.show()","execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the training artifacts\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train_loss','val_loss'], loc = 'upper right')\nplt.show()","execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting on test data.\npred_test = present_model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test_ = encoder.inverse_transform(y_test)","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test_.flatten()\n\ndf.head(10)","execution_count":64,"outputs":[{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"  Predicted Labels Actual Labels\n0             rock          rock\n1            disco        reggae\n2          country       country\n3             jazz          jazz\n4            metal         metal\n5             rock          rock\n6            blues         blues\n7          country       country\n8             rock         disco\n9              pop           pop","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Predicted Labels</th>\n      <th>Actual Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>rock</td>\n      <td>rock</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>disco</td>\n      <td>reggae</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>country</td>\n      <td>country</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>jazz</td>\n      <td>jazz</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>metal</td>\n      <td>metal</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>rock</td>\n      <td>rock</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>blues</td>\n      <td>blues</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>country</td>\n      <td>country</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>rock</td>\n      <td>disco</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>pop</td>\n      <td>pop</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}